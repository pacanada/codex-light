{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# utils\nimport numpy as np\nimport torch\nfrom pathlib import Path\nCHARS =  ['n', 'T', '£', 'W', 'x', 'ù', 'f', 'É', '到', 'æ', '•', '²', '┌', '┤', 'Z', 'S', '…', '/', 'R', 'E', '+', 'o', '∂', 'ü', 'g', '≠', '╫', '午', '‘', '（', 'e', '“', '<', 'ç', '达', '\"', '’', 'ₙ', '#', 'P', '½', 'A', 't', 'X', 'β', 'θ', 'ø', '└', '≡', '║', 'Q', '，', '═', 'h', 'λ', '{', 'ρ', '0', '\\t', 'ö', '：', 'K', '∣', '╩', 'b', 'π', '┬', '.', ')', 'Δ', '?', '8', '7', ']', 'é', 'q', '-', 'v', '┘', ' ', 'c', '∑', 'u', 'p', 'N', '≈', 'C', 'ł', '■', 'Ü', '*', '|', 'U', 'è', '！', 'd', 'm', '│', '–', '┼', \"'\", 'ã', '}', 'y', '_', '→', '\\n', 'z', 'G', 's', '=', 'ů', '2', '!', '\\u2009', '^', 'Y', '⬇', '精', '╥', '4', '9', ':', '下', '≤', '>', '\\\\', 'a', '√', 'V', 'D', ';', 'B', 'î', '~', '￼', '₹', 'L', '—', '(', '5', '↑', 'ñ', 'ò', '├', 'O', 'ń', 'i', 'γ', '┐', '[', '$', 'w', '`', 'µ', '1', 'l', '%', '≥', '─', '✅', '）', 'í', '”', 'j', '−', '€', 'r', 'º', 'ℏ', 'F', '6', '3', ',', 'J', '┴', '&', '×', '@', 'I', 'à', '度', 'φ', '维', 'H', 'k', 'M']\nINDEX_TO_CHAR = {i:c for i, c in enumerate(CHARS)}\nCHAR_TO_INDEX = {c:i for i, c in enumerate(CHARS)}\ndef encode(seq):\n    return [CHAR_TO_INDEX[c] for c in seq ]\n\ndef decode(indexes):\n    return \"\".join([INDEX_TO_CHAR[int(i)] for i in indexes])\n\ndef get_batch(data, batch_size, block_size):\n    idx = torch.randint(len(data)-block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in idx])\n    y = torch.stack([data[i+1:i+1+block_size] for i in idx])\n    return x.cuda(), y.cuda()","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:42:06.689684Z","iopub.execute_input":"2023-04-25T17:42:06.690116Z","iopub.status.idle":"2023-04-25T17:42:10.243995Z","shell.execute_reply.started":"2023-04-25T17:42:06.690079Z","shell.execute_reply":"2023-04-25T17:42:10.242696Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\nfrom dataclasses import dataclass\nimport math\nimport numpy as np\nimport torch.nn as nn\nimport torch\ntorch.manual_seed(0)\n\n@dataclass\nclass Config:\n    block_size = 200\n    n_blocks = 6\n    epochs = int(1e4)\n    vocab_size = 187\n    embedding_dim = 48 # must be equal to head_size in this model but not in example\n    batch_size=256\n    evaluation_steps=300\n    n_head=6\n    learning_rate=0.0003\n    dropout=0.1\n    load_model = True\n    path_model = \"/kaggle/working/v1.pt\"\n    def __post_init__(self):\n        if self.embedding_dim%self.n_head!=0:\n            raise ValueError(f\"Embedding dimension {self.embedding_dim} should be a multiple of n_head={self.n_head}\")\nconfig = Config()\n\nclass Head(nn.Module):\n    def __init__(self, config: Config) -> None:\n        super().__init__()\n        head_size = config.embedding_dim//config.n_head\n        self.key = nn.Linear(config.embedding_dim, head_size, bias=False)\n        self.query = nn.Linear(config.embedding_dim, head_size, bias=False)\n        self.value = nn.Linear(config.embedding_dim, head_size, bias=False)\n        self.register_buffer(\"tril\", torch.tril(torch.ones(config.block_size, config.block_size)))\n        self.dropout = nn.Dropout(config.dropout)\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        v = self.value(x)\n        att = (q @ k.transpose(-2, -1)) *  C**-0.5\n        att = att.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n        att = torch.functional.F.softmax(att, dim=-1)\n        att =self.dropout(att)\n        #att = self.attn_dropout(att)\n        y = att @ v \n        return y\n    \nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias=None):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return torch.functional.F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n    \nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config) -> None:\n        super().__init__()\n        self.heads = nn.ModuleList([Head(config) for _ in  range(config.n_head)])\n        self.proj = nn.Linear(config.embedding_dim, config.embedding_dim)\n        self.dropout = nn.Dropout(config.dropout)\n    def forward(self, x):\n        x = torch.cat([h(x) for h in self.heads], dim=-1)\n        return self.dropout(self.proj(x))\n    \nclass FeedForward(nn.Module):\n    def __init__(self, embedding_dim) -> None:\n        super().__init__()\n        self.l1 = nn.Linear(embedding_dim, 4*embedding_dim, bias=True)\n        self.l2 = nn.Linear(4*embedding_dim, embedding_dim, bias=True)\n        self.dropout=nn.Dropout(0.1)\n    def forward(self, x):\n        x = self.l2(torch.functional.F.relu(self.l1(x)))\n        return self.dropout(x)\n    \nclass Block(nn.Module):\n    def __init__(self, config: Config):\n        super().__init__()\n        self.sa = MultiHeadAttention(config)\n        self.ffw = FeedForward(embedding_dim=config.embedding_dim)\n        self.ln_1 = LayerNorm(config.embedding_dim)\n        self.ln_2 = LayerNorm(config.embedding_dim)\n    def forward(self, x):\n        x = x + self.sa(self.ln_1(x))\n        x = x + self.ffw(self.ln_2(x))\n        return x\n\nclass Transformer(nn.Module):\n    \"\"\"Only considers last token to make predictions\"\"\"\n    def __init__(self, config: Config) -> None:\n        super().__init__()\n        self.config=config\n        self.token_embedding = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.embedding_dim)\n        self.positional_encoding = nn.Embedding(num_embeddings=config.block_size, embedding_dim=config.embedding_dim)\n        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_blocks)],\n            LayerNorm(ndim=config.embedding_dim),\n        )\n        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size, bias=False)\n    def forward(self, x):\n        x = self.token_embedding(x) + self.positional_encoding(torch.arange(self.config.block_size, device=x.device))\n        x = self.blocks(x)\n        x = self.lm_head(x)\n        # Cross entropy already have softmax\n        return x\n\n    def generate_sequence(self, idx, max_tokens, block_size):\n        with torch.no_grad():\n            for i in range(max_tokens):\n                # crop last block_size tokens\n                idx_cond = idx[:,-block_size:]\n                # softmax here is in the other dimension, because we are starting with an actual batch prediction (1,T,vocab)\n                probs = self(idx_cond).softmax(dim=-1)\n                # selecting only last token\n                idx_next = torch.multinomial(input=probs[:,-1,:], num_samples=1)\n                idx = torch.cat((idx, idx_next), dim=-1)\n            return idx.squeeze()\n                \n\n# Loading training and test\ntrain = torch.load(\"/kaggle/input/python-text/train.pt\")\ntest = torch.load(\"/kaggle/input/python-text/test.pt\")\n\nm = Transformer(config).cuda()\n\nif config.load_model:\n    try:\n        m.load_state_dict(torch.load(config.path_model))\n        m.eval()\n        print(f\"Model has been loaded from {config.load_model}\")\n    except:\n        print(\"Could not load model\")\nloss_f = nn.CrossEntropyLoss()\noptim = torch.optim.AdamW(m.parameters(), lr=config.learning_rate)\nfor i in range(config.epochs):\n    X, y = get_batch(train, config.batch_size, config.block_size)\n    \n    out = m(X)\n    optim.zero_grad()\n    # work out the right dimensions for the cross entropy loss function\n    loss = loss_f(out.view(config.batch_size*config.block_size,config.vocab_size), y.view(config.batch_size*config.block_size))\n    loss.backward()\n    optim.step()\n    if i%config.evaluation_steps==0:\n        with torch.no_grad():\n            test_batch_size = config.batch_size\n            X_test, y_test = get_batch(test, test_batch_size, config.block_size)\n            out_test = m(X_test)\n            loss_test = loss_f(out_test.view(test_batch_size*config.block_size,config.vocab_size), y_test.view(test_batch_size*config.block_size))\n        print(\"i: \", i,\" Loss training: \", loss.cpu().item(), \" Loss test: \", loss_test.cpu().item())\n        # starting with \\n\n        #print(decode(m.generate_sequence(torch.tensor((0,0)), 100)))\n        print(decode(m.generate_sequence(X_test[0].view(1,-1), 50, config.block_size)))\n\n        torch.save(m.state_dict(),config.path_model)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:46:23.790224Z","iopub.execute_input":"2023-04-25T17:46:23.790721Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Could not load model\ni:  0  Loss training:  2.888003349304199  Loss test:  2.9092721939086914\nle provides a function which will take a product name as input from the user,\nand fetch from Amazon information about products of this name or category.  The product\ninformation will include title, URosorod}\" bonnarets)\n ctatsideay ird p cout attidut\ni:  300  Loss training:  1.7995688915252686  Loss test:  2.087390661239624\n\n\ndef stooge(arr, i, h):\n    if i >= h:\n        return\n\n    # If first element is smaller than the last then swap them\n    if arr[i] > arr[h]:\n        arr[i], arr[h] = arr[h], arr[i]\n\n    # If there ateeat corsemed(clenumpy: [st: roust - Int_sum* alb\ni:  600  Loss training:  1.728990077972412  Loss test:  1.926145076751709\ner = 0\n        digits = int(digits)\n        while counter < digits:\n            current = temp_num % 10\n            if counter % 2 == 0:\n                addition = \"\"\n                if counter in plass\n\n      if n!==[letr[inke] 1\n          self.prer\ni:  900  Loss training:  1.688098430633545  Loss test:  1.9088873863220215\nivot)\n\n    \"\"\"\n    Insert the remaining items.\n    In this case, 40 < 75 is sure because it has already been sorted.\n    Therefore, you only need to insert 75 into [100, 999, 10000],\n    so that you chin p(whilab ttorijale(teaterd***|, que: thamb, w \ni:  1200  Loss training:  1.6716973781585693  Loss test:  1.9071766138076782\n cover\n    if USER_TOKEN:\n        for key, value in fetch_github_info(USER_TOKEN).items():\n            print(f\"{key}: {value}\")\n    else:\n        raise ValueError(\"'USER_TOKEN' field cannot be empty.\", nuff da it, k: tresofe Nthees wt airghth\n\n    de\ni:  1500  Loss training:  1.631556749343872  Loss test:  1.9498498439788818\nssical bits using one quantum\nbit. This circuit is designed using the Qiskit\nframework. This experiment run in IBM Q simulator\nwith 1000 shots.\n.\nReferences:\nhttps://qiskit.org/textbook/ch-algorithms/31_vrte_Bu rcytet to_pablectidionan C Secerencecte\ni:  1800  Loss training:  1.6680036783218384  Loss test:  1.8536838293075562\n])\n    [[-1, -0.5], [-1, -1.5]]\n    >>> subtract([3], [4, 5])\n    Traceback (most recent call last):\n      ...\n    TypeError: Expected a matrix, got int/list instead\n    \"\"\"\n    if (\n        _check_noum_freque = 2:\n,\n           piranters\n   whhilede \ni:  2100  Loss training:  1.6107673645019531  Loss test:  1.8633151054382324\n, 12]\n\n    >>> partition(array, 0, len(array), 12)\n    8\n    \"\"\"\n    i = low\n    j = high\n    while True:\n        while array[i] < pivot:\n            i += 1\n        j -= 1\n        while pivot < array[1] + rerange(2,\n         #  wrigh Aricus vitorpone\ni:  2400  Loss training:  1.5986932516098022  Loss test:  1.8273627758026123\n shell_sort(collection):\n    \"\"\"Pure implementation of shell sort algorithm in Python\n    :param collection:  Some mutable ordered collection with heterogeneous\n    comparable items inside\n    :return the pare of th stardun thelar ton) or no mathe n \ni:  2700  Loss training:  1.6044225692749023  Loss test:  1.8403053283691406\nquence, start, mid)\n    slowsort(sequence, mid + 1, end)\n\n    if sequence[end] < sequence[mid]:\n        sequence[end], sequence[mid] = sequence[mid], sequence[end]\n\n    slowsort(sequence, start, end -7, tostages, print(number 4,39), (2, 7), (x))\n    \ni:  3000  Loss training:  1.6010397672653198  Loss test:  1.8467140197753906\n>>> insertion_sort([-2, -5, -45]) == sorted([-2, -5, -45])\n    True\n    >>> insertion_sort(['d', 'a', 'b', 'e', 'c']) == sorted(['d', 'a', 'b', 'e', 'c'])\n    True\n    >>> import random\n    >>> collection('Tragesttrinance(\"d'bd\")\n    TreeNode\n    Tra\ni:  3300  Loss training:  1.5448991060256958  Loss test:  1.7963740825653076\nse_row(reverse_column(matrix))\n    # OR.. reverse_column(reverse_row(matrix))\n\n\ndef rotate_270(matrix: list[list[int]]) -> list[list[int]]:\n    \"\"\"\n    >>> rotate_270(make_matrix())\n    [[13, 9, 5, 1], 4: {gine.mos_inver(1)], colvel tumbe) < float),\n\ni:  3600  Loss training:  1.5816562175750732  Loss test:  1.7617323398590088\n\"\"\n    if any(not isinstance(x, int) or x < 0 for x in sequence):\n        raise TypeError(\"Sequence must be list of non-negative integers\")\n    for _ in range(len(sequence)):\n        for i, (rod_upper_serity)).\n    if node:\n           next[i, t] = le\ni:  3900  Loss training:  1.5541645288467407  Loss test:  1.7963688373565674\nr elements in the right half of the array\n            arr[index] = right_array[right_index]\n            right_index += 1\n            index += 1\n    return arr\n\n\nif __name__ == \"__main__\":\n    import deft_copimal_noders:\n    treeturns Noneser onsed\n\n\n\ni:  4200  Loss training:  1.5653202533721924  Loss test:  1.8595889806747437\nction run by the processes that sorts the list\n\nposition = the position in the list the process represents, used to know which\n            neighbor we pass our value to\nvalue = the initial value at like a Mbj maj.\n\n    \"\"\"\n    cisimale thape sepoblem\ni:  4500  Loss training:  1.545335054397583  Loss test:  1.8260557651519775\n    right = current_row[col + 1]\n            diagonal = next_row[col + 1]\n            bottom = next_row[col]\n\n            if mat[row][col] == 1:\n                current_row[col] = 1 + min(right, diagoriboullus) err efcounce2tion,\n               m + n\ni:  4800  Loss training:  1.4727061986923218  Loss test:  1.7201358079910278\n+ left + right\n    return input_list\n\n\n# iteration over the unsorted list\ndef iter_merge_sort(input_list: list) -> list:\n    \"\"\"\n    Return a sorted copy of the input list\n\n    >>> iter_merge_sort([5,10], 1)\n    (1166201)\n     >>> j = q [-12, 441, 40\ni:  5100  Loss training:  1.535047173500061  Loss test:  1.8624091148376465\n  for i in range(0, 10):\n        if (i + position) % 2 == 0 and r_send is not None:\n            # send your value to your right neighbor\n            process_lock.acquire()\n            r_send[1].send(v).l()[1])\n             self, bock, anvfs_p, energr\n","output_type":"stream"}]}]}